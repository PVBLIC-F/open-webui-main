"""
Gmail Indexer V2 - Optimized Single-Vector Strategy

Production-grade email indexing optimized for semantic search at scale.

Key Improvements:
- Single high-quality vector per email (not 6+ chunks)
- Advanced text cleaning for better embeddings
- Rich metadata for precise filtering
- Optimized for 100,000+ email repositories
- 6x faster search, 6x lower costs vs multi-chunk approach
- Integrates with admin panel Document/RAG settings

Architecture:
- One email = One vector = One search result
- Metadata-rich for filtering (sender, date, labels, etc.)
- Uses admin-configured embedding model and batch size
- Respects embedding model token limits
- Clean, readable code for maintainability
"""

import logging
import time
import asyncio
import re
import hashlib
from datetime import datetime
from typing import Dict, List, Optional

from open_webui.utils.gmail_processor import GmailProcessor
from open_webui.routers.retrieval import extract_enhanced_metadata
from open_webui.config import (
    RAG_EMBEDDING_BATCH_SIZE,
    SEMANTIC_MAX_CHUNK_SIZE,
)

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


class GmailIndexerV2:
    """
    Optimized email indexer using single-vector strategy.
    
    Performance Characteristics:
    - 10,000 emails = 10,000 vectors (vs 60,000 with chunking)
    - Average processing: 10-30 emails/second (with embeddings)
    - Average search: <100ms for 100,000+ emails
    - Storage: ~1.5KB per email (vector + metadata)
    
    Note: Processing speed depends on:
    - Embedding model (local vs API)
    - Attachment processing enabled
    - Network latency
    - Hardware (CPU/GPU)
    """
    
    # Index version for tracking and A/B testing
    INDEX_VERSION = "v2-single-vector"
    
    def __init__(
        self,
        embedding_service,
        document_processor,
        app_config=None,
    ):
        """
        Initialize optimized indexer.
        
        Args:
            embedding_service: EmbeddingService from Open WebUI
            document_processor: DocumentProcessor for quality scoring
            app_config: Optional app config for accessing RAG settings
        """
        self.gmail_processor = GmailProcessor()
        self.embeddings = embedding_service
        self.doc_processor = document_processor
        self.app_config = app_config
        
        # Get optimal text length from admin settings
        # Use SEMANTIC_MAX_CHUNK_SIZE but cap at safe limits for default model
        # Default model (all-MiniLM-L6-v2): 256 tokens max (~800-1000 chars)
        # OpenAI (text-embedding-3-small): 8191 tokens (~32k chars)
        
        semantic_max = SEMANTIC_MAX_CHUNK_SIZE.value if SEMANTIC_MAX_CHUNK_SIZE else 2000
        
        # For email embeddings, be conservative
        # If admin set high chunk size (for docs), cap it at safe email limit
        self.OPTIMAL_TEXT_LENGTH = min(semantic_max, 800)  # chars
        self.MAX_TEXT_LENGTH = min(semantic_max + 400, 1200)  # with buffer
        
        # Get batch size from settings
        self.BATCH_SIZE = RAG_EMBEDDING_BATCH_SIZE.value if RAG_EMBEDDING_BATCH_SIZE else 1
        
        logger.info(f"GmailIndexerV2 initialized - {self.INDEX_VERSION}")
        logger.info(f"  Text limits: optimal={self.OPTIMAL_TEXT_LENGTH}, max={self.MAX_TEXT_LENGTH}")
        logger.info(f"  Embedding batch size: {self.BATCH_SIZE}")
        logger.info(f"  Using admin RAG settings: SEMANTIC_MAX_CHUNK_SIZE={semantic_max}")

    async def process_email_for_indexing(
        self,
        email_data: dict,
        user_id: str,
        fetcher=None,
        process_attachments: bool = True,
        max_attachment_size_mb: int = 10,
        allowed_attachment_types: str = ".pdf,.docx,.doc,.xlsx,.xls,.pptx,.ppt,.txt,.csv,.md,.html,.eml",
    ) -> Dict:
        """
        Process a single email into optimized single-vector format.
        
        Pipeline:
        1. Parse email â†’ structured data
        2. Process attachments (if enabled)
        3. Deep clean text â†’ embedding-ready format
        4. Create optimal search text
        5. Generate ONE high-quality embedding
        6. Build rich metadata
        7. Return upsert-ready data
        
        Args:
            email_data: Gmail API message response
            user_id: User ID for isolation
            fetcher: GmailFetcher for attachment downloads
            process_attachments: Enable attachment processing
            max_attachment_size_mb: Max attachment size
            allowed_attachment_types: Allowed file extensions
            
        Returns:
            Dict with upsert_data ready for vector DB
        """
        start_time = time.time()
        
        # Step 1: Parse email into structured format
        parsed = self.gmail_processor.parse_email(email_data, user_id)
        email_id = parsed["email_id"]
        base_metadata = parsed["metadata"]
        
        logger.info(
            f"Processing email {email_id[:8]}... "
            f"({base_metadata['subject'][:50]}...)"
        )
        
        # Step 2: Process attachments (if enabled)
        attachment_texts = []
        attachment_names = []
        
        if process_attachments and parsed.get("attachments") and fetcher:
            attachment_results = await self._process_email_attachments(
                parsed["attachments"],
                fetcher,
                email_id,
                max_attachment_size_mb,
                allowed_attachment_types
            )
            attachment_texts = [att["text"] for att in attachment_results]
            attachment_names = [att["filename"] for att in attachment_results]
            
            if attachment_names:
                logger.info(f"  ðŸ“Ž Processed {len(attachment_names)} attachment(s)")
        
        # Step 3: Create optimal search text (what gets embedded)
        search_text = self._create_optimal_search_text(
            subject=base_metadata["subject"],
            from_name=base_metadata["from_name"],
            body=parsed["document_text"],
            attachment_texts=attachment_texts,
        )
        
        # Debug: Check if search_text is empty
        if not search_text or not search_text.strip():
            logger.warning(
                f"  âš ï¸ Email {email_id[:8]} has empty search_text after cleaning! "
                f"Subject: {base_metadata['subject'][:50]}, "
                f"Body length: {len(parsed['document_text'])}"
            )
            # Fallback: use cleaned subject + snippet if body is empty
            search_text = f"{base_metadata['subject']}\n\n{parsed.get('snippet', '')}"
            search_text = self._deep_clean_for_embeddings(search_text)
        
        logger.debug(f"  Search text length: {len(search_text)} chars")
        logger.debug(f"  Search text preview: {search_text[:200]}...")
        
        # Step 4: Extract enhanced metadata for filtering/enrichment
        # CRITICAL: Clean the text one more time before metadata extraction
        # This ensures extract_enhanced_metadata receives clean text without escape sequences
        search_text_for_metadata = self._final_clean_before_metadata(search_text)
        
        # Remove "Subject:" and "From:" prefixes for better entity extraction
        # These labels confuse the entity extraction algorithm
        search_text_for_metadata = re.sub(r'^Subject:\s*', '', search_text_for_metadata)
        search_text_for_metadata = re.sub(r'\s+From:\s+', '\n', search_text_for_metadata)
        search_text_for_metadata = search_text_for_metadata.strip()
        
        logger.debug(f"  Text for metadata extraction (cleaned): {search_text_for_metadata[:200]}...")
        
        # Use LLM=False for speed (rule-based extraction)
        enhanced_metadata = extract_enhanced_metadata(search_text_for_metadata, use_llm=False)
        
        # Debug: Check enhanced metadata quality AND content
        logger.debug(
            f"  Enhanced metadata: "
            f"topics={len(enhanced_metadata.get('topics', []))}, "
            f"keywords={len(enhanced_metadata.get('keywords', []))}, "
            f"entities_people={len(enhanced_metadata.get('entities_people', []))}"
        )
        logger.debug(f"  RAW entity_people from extract: {enhanced_metadata.get('entities_people', [])}")
        logger.debug(f"  RAW topics from extract: {enhanced_metadata.get('topics', [])}")
        logger.debug(f"  RAW entity_locations from extract: {enhanced_metadata.get('entities_locations', [])}")
        
        # Step 5: Generate ONE high-quality embedding
        # Note: embedding service only has embed_batch, so we pass single-item list
        embeddings = await self.embeddings.embed_batch([search_text])
        embedding = embeddings[0] if embeddings else [0.0] * 1536
        
        # Step 6: Calculate quality score
        quality_score = self.doc_processor.quick_quality_score(search_text)
        
        # Step 7: Build comprehensive metadata
        metadata = self._build_optimized_metadata(
            base_metadata=base_metadata,
            enhanced_metadata=enhanced_metadata,
            search_text=search_text,
            quality_score=quality_score,
            attachment_names=attachment_names,
        )
        
        # Step 8: Create upsert data
        upsert_data = [{
            "id": f"email-{email_id}",
            "values": [float(v) if v is not None else 0.0 for v in embedding],
            "metadata": metadata,
        }]
        
        processing_time = time.time() - start_time
        
        logger.info(
            f"  âœ… Email {email_id[:8]} indexed: "
            f"1 vector, quality={quality_score}, {processing_time:.2f}s"
        )
        
        return {
            "email_id": email_id,
            "upsert_data": upsert_data,
            "vectors_created": 1,
            "processing_time": processing_time,
            "quality_score": quality_score,
        }

    async def process_email_batch(
        self,
        emails: List[dict],
        user_id: str,
        fetcher=None,
        process_attachments: bool = True,
        max_attachment_size_mb: int = 10,
        allowed_attachment_types: str = ".pdf,.docx,.doc,.xlsx,.xls,.pptx,.ppt,.txt,.csv,.md,.html,.eml",
    ) -> Dict:
        """
        Process multiple emails in batch with deduplication.
        
        Performance: 10-30 emails/second (depends on embedding model/hardware)
        
        Deduplication: Detects mass emails by comparing subject + snippet hash
        
        Returns:
            Dict with batch statistics and upsert data
        """
        batch_start = time.time()
        all_upsert_data = []
        processed_count = 0
        error_count = 0
        duplicate_count = 0
        total_quality_score = 0
        
        # Track content fingerprints for mass email detection
        seen_content = {}
        
        logger.info(f"ðŸ“¦ Processing batch of {len(emails)} emails")
        
        for email_data in emails:
            try:
                email_id = email_data.get("id", "unknown")
                snippet = email_data.get("snippet", "")
                
                # Detect mass emails (same content sent to many recipients)
                # Use subject + first 100 chars of snippet for more accurate deduplication
                headers = email_data.get("payload", {}).get("headers", [])
                subject = next((h["value"] for h in headers if h["name"] == "Subject"), "")
                content_fingerprint = hashlib.md5(
                    f"{subject[:100]}{snippet[:100]}".encode()
                ).hexdigest()
                
                if content_fingerprint in seen_content:
                    duplicate_count += 1
                    original_id = seen_content[content_fingerprint]
                    logger.debug(
                        f"  â­ï¸  Skipping duplicate: {email_id[:8]} "
                        f"(same as {original_id[:8]})"
                    )
                    continue
                
                seen_content[content_fingerprint] = email_id
                
                # Process email
                result = await self.process_email_for_indexing(
                    email_data,
                    user_id,
                    fetcher=fetcher,
                    process_attachments=process_attachments,
                    max_attachment_size_mb=max_attachment_size_mb,
                    allowed_attachment_types=allowed_attachment_types,
                )
                
                all_upsert_data.extend(result["upsert_data"])
                processed_count += 1
                total_quality_score += result.get("quality_score", 0)
                
                # Clear email data to free memory
                email_data.clear()
                
                # Yield to event loop every 10 emails
                if processed_count % 10 == 0:
                    await asyncio.sleep(0)
                    
            except Exception as e:
                email_id = email_data.get("id", "unknown")
                logger.error(f"  âŒ Error processing {email_id[:8]}: {e}")
                error_count += 1
                continue
        
        batch_time = time.time() - batch_start
        avg_quality = total_quality_score / processed_count if processed_count > 0 else 0
        emails_per_second = processed_count / batch_time if batch_time > 0 else 0
        
        logger.info(
            f"âœ… Batch complete: {processed_count} emails, "
            f"{duplicate_count} duplicates, {error_count} errors, "
            f"{emails_per_second:.1f} emails/s, avg quality={avg_quality:.1f}"
        )
        
        return {
            "processed": processed_count,
            "duplicates_skipped": duplicate_count,
            "errors": error_count,
            "total_vectors": len(all_upsert_data),
            "upsert_data": all_upsert_data,
            "processing_time": batch_time,
            "avg_quality_score": avg_quality,
        }

    def _create_optimal_search_text(
        self,
        subject: str,
        from_name: str,
        body: str,
        attachment_texts: List[str] = None,
    ) -> str:
        """
        Create optimal text for semantic embedding.
        
        This is THE most important function for search quality.
        The text created here directly determines how well semantic search works.
        
        Strategy:
        - Subject gets priority (most semantically important)
        - From name for "emails from X" queries
        - Body is deeply cleaned and optimized
        - Attachments are summarized if present
        - Final length optimized for embedding model
        
        Returns:
            Clean, semantically-rich text optimized for neural embeddings
        """
        parts = []
        
        # DEBUG: Log inputs
        logger.debug(f"Creating search text - Subject: {len(subject)} chars, From: {len(from_name)} chars, Body: {len(body)} chars")
        
        # 1. Subject (highest semantic importance)
        if subject and subject != "(No Subject)":
            parts.append(f"Subject: {subject}")
            logger.debug(f"  Added subject: {len(subject)} chars")
        
        # 2. From name (important for person-based queries)
        if from_name:
            parts.append(f"From: {from_name}")
            logger.debug(f"  Added from_name: {len(from_name)} chars")
        
        # 3. Email body (deeply cleaned)
        if body:
            # DEBUG: Check for escape sequences in input
            has_escapes = '\\n' in body or '\\t' in body or '\\r' in body
            logger.debug(f"  Body has escape sequences: {has_escapes}, Body preview: {body[:100]}")
            
            body_clean = self._deep_clean_for_embeddings(body)
            
            # Debug: Log cleaning impact
            logger.debug(f"  Body: {len(body)} chars â†’ Cleaned: {len(body_clean)} chars")
            if len(body) > 0 and len(body_clean) == 0:
                logger.warning(
                    f"  âš ï¸ Body cleaning removed ALL content! "
                    f"Original: {len(body)} chars â†’ Cleaned: {len(body_clean)} chars"
                )
            
            # Limit body length intelligently
            if len(body_clean) > self.OPTIMAL_TEXT_LENGTH:
                # Keep first N chars (usually most important content)
                body_clean = body_clean[:self.OPTIMAL_TEXT_LENGTH]
                # Find last complete sentence
                last_period = body_clean.rfind('. ')
                if last_period > self.OPTIMAL_TEXT_LENGTH * 0.8:
                    body_clean = body_clean[:last_period + 1]
            
            if body_clean:
                parts.append(body_clean)
            elif body:
                # Fallback: if cleaning removed everything, use first 800 chars of original
                logger.warning("Cleaning removed all body text, using original (lightly cleaned)")
                body_fallback = body[:self.OPTIMAL_TEXT_LENGTH]
                # Just remove obvious escape sequences
                body_fallback = body_fallback.replace('\\n', '\n').replace('\\t', ' ')
                parts.append(body_fallback)
        
        # 4. Attachment content (if substantive)
        if attachment_texts:
            combined_attachments = " ".join(attachment_texts)
            # Clean and limit attachment text (leave room for subject + body)
            att_clean = self._deep_clean_for_embeddings(combined_attachments)
            # Limit to 200 chars to ensure total stays under MAX_TEXT_LENGTH
            if len(att_clean) > 200:
                att_clean = att_clean[:200] + "..."
            
            if att_clean:
                parts.append(f"Attachments: {att_clean}")
        
        # Debug: Log parts before joining
        logger.debug(f"  Parts to combine: {len(parts)} parts")
        for i, part in enumerate(parts):
            has_escapes = '\\n' in part
            logger.debug(f"    Part {i}: {len(part)} chars, has escapes: {has_escapes}")
        
        # Combine with proper spacing
        final_text = "\n\n".join(parts)
        
        has_escapes_after_join = '\\n' in final_text
        logger.debug(f"  After join: {len(final_text)} chars, has escapes: {has_escapes_after_join}")
        
        # Final length check
        if len(final_text) > self.MAX_TEXT_LENGTH:
            final_text = final_text[:self.MAX_TEXT_LENGTH]
        
        final_text = final_text.strip()
        
        # Sanity check: ensure no literal escape sequences remain
        if '\\n' in final_text or '\\t' in final_text or '\\r' in final_text:
            logger.warning(f"  âš ï¸ Search text has escape sequences! Cleaning now...")
            logger.debug(f"  Before escape clean: {repr(final_text[:100])}")
            # One more pass to clean escape sequences
            final_text = final_text.replace('\\n', '\n').replace('\\t', ' ')
            final_text = final_text.replace('\\r', '').replace('\\\\', '')
            # Re-normalize whitespace after converting escapes
            final_text = re.sub(r'\s+', ' ', final_text)
            final_text = re.sub(r'\n{3,}', '\n\n', final_text)
            final_text = final_text.strip()
            logger.debug(f"  After escape clean: {repr(final_text[:100])}")
        
        logger.debug(f"  Final search text: {len(final_text)} chars")
        
        return final_text

    @staticmethod
    def _deep_clean_for_embeddings(text: str) -> str:
        """
        Production-grade text cleaning for optimal embeddings.
        
        This is DEFENSIVE cleaning - even though gmail_processor does basic cleaning,
        we apply comprehensive cleaning here to ensure optimal embedding quality.
        
        This function eliminates ALL artifacts that degrade embedding quality:
        - Escape sequences (\n\n, \", etc.) - Often present in API responses
        - URLs and tracking links - Semantic noise
        - Email addresses - Privacy + reduces noise
        - Quoted reply text - Redundant content
        - Email signatures - Not searchable content
        - Control characters - Can break embeddings
        - Excessive whitespace - Wastes token budget
        - Line break artifacts - Improve readability
        
        Returns:
            Clean, natural text optimized for neural network understanding
        """
        if not text:
            return ""
        
        # STEP 1: Fix escape sequences (critical for embedding quality)
        # These MUST be handled first - handle both single and double-escaped
        
        # First, handle double-escaped sequences (\\n â†’ \n)
        text = text.replace('\\\\n', '\n')
        text = text.replace('\\\\t', '\t')
        text = text.replace('\\\\r', '\r')
        text = text.replace('\\\\\\\\', '\\')
        
        # Then handle single-escaped sequences (\n â†’ actual newline)
        text = text.replace('\\n\\n', '\n\n')
        text = text.replace('\\n', '\n')
        text = text.replace('\\t', '    ')  # Convert tabs to spaces
        text = text.replace('\\r', '')
        text = text.replace('\\"', '"')
        text = text.replace("\\'", "'")
        text = text.replace('\\\\', '')
        text = text.replace('\\/', '/')
        
        # STEP 2: Remove quoted reply blocks (noise for search)
        # "On Mon, Oct 14, 2024, John wrote:" and everything after
        text = re.sub(
            r'On .{1,100}wrote:.*',
            '',
            text,
            flags=re.DOTALL | re.IGNORECASE
        )
        
        # Remove lines starting with > (quoted text)
        text = re.sub(r'^>.*$', '', text, flags=re.MULTILINE)
        
        # Remove Outlook-style forwarded headers
        text = re.sub(
            r'From:.*?\nSent:.*?\nTo:.*?\nSubject:.*?\n',
            '',
            text,
            flags=re.DOTALL | re.IGNORECASE
        )
        
        # STEP 3: Remove email signatures
        # Standard signature delimiter
        text = re.sub(r'--\s*\n.*', '', text, flags=re.DOTALL)
        
        # "Sent from my iPhone" etc
        text = re.sub(
            r'Sent from my (iPhone|iPad|Android|Mobile).*',
            '',
            text,
            flags=re.IGNORECASE | re.DOTALL
        )
        
        # Common signature patterns
        signature_markers = [
            r'Thanks?,?\s*\n[A-Z][a-z]+\s*$',  # "Thanks, John"
            r'Best,?\s*\n[A-Z][a-z]+\s*$',     # "Best, Sarah"
            r'Regards?,?\s*\n[A-Z][a-z]+\s*$', # "Regards, Mike"
        ]
        for pattern in signature_markers:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # STEP 4: Remove URLs (they're semantic noise)
        text = re.sub(
            r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
            '[link]',
            text
        )
        
        # STEP 5: Remove email addresses (privacy + noise reduction)
        text = re.sub(
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            '[email]',
            text
        )
        
        # STEP 6: Fix line break artifacts (critical for readability)
        # Hyphenated line breaks: "discus-\nsion" â†’ "discussion"
        text = re.sub(r'(\w)-\s*\n\s*(\w)', r'\1\2', text)
        
        # Word wrapping: "to\nshare" â†’ "to share"
        text = re.sub(r'(?<=\w)\n(?=\w)', ' ', text)
        
        # STEP 7: Remove HTML entities
        from html import unescape
        text = unescape(text)
        
        # STEP 8: Remove control characters (except newlines/spaces)
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F]', '', text)
        
        # STEP 9: Normalize whitespace
        # Multiple spaces â†’ single space
        text = re.sub(r' {2,}', ' ', text)
        
        # Excessive newlines â†’ max 2
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # Trim whitespace from each line
        lines = text.split('\n')
        text = '\n'.join(line.strip() for line in lines)
        
        # STEP 10: Fix orphaned punctuation
        text = re.sub(r'\s+([.,!?;:])', r'\1', text)  # " ." â†’ "."
        text = re.sub(r'([.,!?;:])([A-Za-z])', r'\1 \2', text)  # ".Hi" â†’ ". Hi"
        
        # STEP 11: Remove confidentiality notices and disclaimers
        text = re.sub(
            r'(This email|This message|CONFIDENTIAL|DISCLAIMER).*?(intended recipient|confidential).*?\.',
            '',
            text,
            flags=re.IGNORECASE | re.DOTALL
        )
        
        # Final trim
        text = text.strip()
        
        return text

    def _build_optimized_metadata(
        self,
        base_metadata: Dict,
        enhanced_metadata: Dict,
        search_text: str,
        quality_score: int,
        attachment_names: List[str] = None,
    ) -> Dict:
        """
        Build comprehensive metadata for filtering and display.
        
        Metadata Strategy:
        - Structured fields for precise filtering
        - Searchable text snippet for result display
        - Enhanced fields for faceted search
        - Version tracking for A/B testing
        
        Returns:
            Complete metadata dict ready for vector DB
        """
        email_id = base_metadata["email_id"]
        user_id = base_metadata["user_id"]
        
        # Create clean text snippet for display (first 500 chars)
        text_snippet = search_text[:500].strip()
        if len(search_text) > 500:
            text_snippet += "..."
        
        # CRITICAL FIX: If text_snippet is empty, use subject as fallback
        if not text_snippet or len(text_snippet) < 10:
            logger.warning(
                f"  âš ï¸ text_snippet empty or too short ({len(text_snippet)} chars), "
                f"using subject as fallback"
            )
            text_snippet = base_metadata["subject"]
            if not text_snippet:
                text_snippet = f"Email {email_id[:8]}"
        
        # Apply final Pinecone cleaning (matches chat summary format)
        text_snippet_clean = self._clean_for_pinecone(text_snippet)
        
        # Remove "Subject:" and "From:" prefixes from display text
        # These are useful for embeddings but clutter display
        text_snippet_clean = re.sub(r'^Subject:\s*', '', text_snippet_clean)
        text_snippet_clean = re.sub(r'\s+From:\s+', ' ', text_snippet_clean)
        
        # Remove email reply/forward abbreviations
        # Re:, RE:, Fwd:, FW:, etc. - these are just noise
        text_snippet_clean = re.sub(r'\b(Re|RE|Fwd|FW|Fw):\s*', '', text_snippet_clean, flags=re.IGNORECASE)
        
        # Clean up any duplicate content (subject appearing twice in some emails)
        # Split by spaces and remove exact duplicates while preserving order
        words = text_snippet_clean.split()
        seen = set()
        clean_words = []
        for word in words:
            word_lower = word.lower()
            if word_lower not in seen or len(word_lower) <= 3:  # Keep short words even if repeated
                clean_words.append(word)
                seen.add(word_lower)
        text_snippet_clean = ' '.join(clean_words)
        
        text_snippet_clean = text_snippet_clean.strip()
        
        # Debug: Log text_snippet creation
        logger.debug(
            f"  Text snippet: search_text={len(search_text)} chars, "
            f"snippet={len(text_snippet_clean)} chars, "
            f"preview: {text_snippet_clean[:100]}"
        )
        
        # Final sanity check before building metadata
        logger.debug(f"  Building metadata with text_snippet_clean: {len(text_snippet_clean)} chars")
        
        metadata = {
            # === Core Identification ===
            "type": "email",
            "email_id": email_id,
            "thread_id": base_metadata["thread_id"],
            "user_id": user_id,
            "index_version": self.INDEX_VERSION,
            
            # === Searchable Text (PRIMARY - matches chat summary format) ===
            # Use chunk_text as primary field (standard for all Open WebUI content)
            # Fallback to text for backward compatibility
            "chunk_text": text_snippet_clean,  # PRIMARY: Used for search/display (Pinecone-cleaned)
            "text": text_snippet_clean,  # FALLBACK: For compatibility
            
            # === Structured Email Fields (for filtering) ===
            "subject": base_metadata["subject"],
            "from_name": base_metadata["from_name"],
            "from_email": base_metadata["from_email"],
            "to_name": base_metadata.get("to_name", ""),
            "to_email": base_metadata.get("to_email", ""),
            "cc": base_metadata.get("cc", ""),
            
            # === Temporal Fields (critical for filtering) ===
            "date": base_metadata["date"],
            "date_timestamp": base_metadata["date_timestamp"],
            "timestamp": datetime.utcnow().isoformat() + "Z",
            
            # === Gmail Labels (for filtering) ===
            "labels": (
                ",".join(base_metadata["labels"])
                if isinstance(base_metadata["labels"], list)
                else base_metadata["labels"]
            ),
            
            # === Email Properties ===
            "has_attachments": base_metadata["has_attachments"],
            "attachment_count": len(attachment_names) if attachment_names else 0,
            "attachment_names": ",".join(attachment_names) if attachment_names else "",
            "is_reply": base_metadata.get("is_reply", False),
            "word_count": len(search_text.split()),
            
            # === Quality Metrics ===
            "quality_score": quality_score,
            "vector_dim": 1536,  # Standard OpenAI embedding dimension
            
            # === Chunk Information (matches chat summary format) ===
            "chunk_index": 0,  # Single vector per email
            "total_chunks": 1,  # No chunking in V2
            
            # === Enhanced Metadata (from extract_enhanced_metadata) ===
            # Clean each field to remove escape sequences
            "topics": self._clean_metadata_field(
                self._extract_list_values(enhanced_metadata.get("topics", []))
            ),
            "keywords": self._clean_metadata_field(
                self._extract_list_values(enhanced_metadata.get("keywords", []))
            ),
            "entity_people": self._clean_metadata_field(
                self._extract_list_values(enhanced_metadata.get("entities_people", []))
            ),
            "entity_organizations": self._clean_metadata_field(
                self._extract_list_values(enhanced_metadata.get("entities_organizations", []))
            ),
            "entity_locations": self._clean_metadata_field(
                self._extract_list_values(enhanced_metadata.get("entities_locations", []))
            ),
            
            # === Source & Type ===
            "source": "gmail",
            "doc_type": "email",
            
            # === Summary/Record ID (matches chat summary format) ===
            "summary_id": f"email-{email_id}",  # Consistent with chat summaries
            
            # === Deduplication ===
            "hash": hashlib.sha256(
                f"{email_id}{user_id}".encode()
            ).hexdigest(),
        }
        
        # FINAL DEBUG: Verify text fields are set
        logger.debug(
            f"  Metadata built: chunk_text={len(metadata['chunk_text'])} chars, "
            f"text={len(metadata['text'])} chars, "
            f"subject={len(metadata['subject'])} chars, "
            f"word_count={metadata['word_count']}"
        )
        
        return metadata

    @staticmethod
    def _clean_metadata_field(text: str) -> str:
        """
        Aggressively clean a metadata field value.
        
        This removes ALL escape sequences and normalizes text for clean storage.
        Applied to entity_people, topics, keywords, etc.
        """
        if not text:
            return ""
        
        # Remove ALL forms of escape sequences
        text = text.replace('\\n\\n', ' ')  # Double newline
        text = text.replace('\\n', ' ')     # Single newline
        text = text.replace('\\t', ' ')     # Tab
        text = text.replace('\\r', '')      # Carriage return
        text = text.replace('\\\\', '')     # Backslash
        text = text.replace('\\"', '"')     # Escaped quote
        text = text.replace("\\'", "'")     # Escaped single quote
        
        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    @staticmethod
    def _final_clean_before_metadata(text: str) -> str:
        """
        Final cleaning pass specifically for metadata extraction.
        
        This ensures extract_enhanced_metadata() receives perfectly clean text
        without ANY escape sequences, so entity/topic extraction works correctly.
        
        Returns:
            Clean text with actual newlines (not escape sequences)
        """
        if not text:
            return ""
        
        # Handle ALL escape sequence variations
        text = text.replace('\\n\\n', '\n\n')  # Double escaped newline
        text = text.replace('\\n', '\n')       # Single escaped newline
        text = text.replace('\\t', '\t')       # Escaped tab
        text = text.replace('\\r', '')         # Escaped carriage return
        text = text.replace('\\"', '"')        # Escaped quote
        text = text.replace("\\'", "'")        # Escaped single quote
        text = text.replace('\\\\', '')        # Escaped backslash
        
        # Normalize whitespace but KEEP newlines for metadata extraction
        text = re.sub(r' {2,}', ' ', text)     # Multiple spaces â†’ single
        text = re.sub(r'\n{3,}', '\n\n', text) # Max 2 newlines
        
        return text.strip()
    
    @staticmethod
    def _clean_for_pinecone(text: str) -> str:
        """
        Final cleaning for Pinecone storage (matches TextCleaner.clean_for_pinecone).
        
        This is CRITICAL for metadata storage:
        - Converts ANY remaining escape sequences to actual chars
        - Removes control characters (including newlines for single-line storage)
        - Normalizes to single-line format with PROPER space separation
        - Ensures valid UTF-8
        
        Returns single-line text suitable for Pinecone metadata storage.
        """
        if not text:
            return ""
        
        # STEP 0: Handle any remaining escape sequences (defensive)
        # This catches any escape sequences that slipped through earlier cleaning
        text = text.replace('\\n\\n', '  ')  # Double newline â†’ double space (paragraph break)
        text = text.replace('\\n', ' ')  # Single newline â†’ single space
        text = text.replace('\\t', ' ')  # Tab â†’ space
        text = text.replace('\\r', '')   # Remove carriage return
        text = text.replace('\\\\', '')  # Remove literal backslash
        
        # STEP 1: Convert ACTUAL newlines to spaces BEFORE removing control chars
        # This ensures proper word spacing (critical fix!)
        text = text.replace('\n\n', '  ')  # Paragraph break â†’ double space
        text = text.replace('\n', ' ')     # Line break â†’ single space
        text = text.replace('\t', ' ')     # Tab â†’ space
        
        # STEP 2: Remove remaining control characters and zero-width chars
        text = re.sub(r"[\x00-\x1F\x7F-\x9F\u200B-\u200D\uFEFF]", "", text)
        
        # STEP 3: Normalize whitespace to single spaces
        text = re.sub(r" {2,}", " ", text)  # Multiple spaces â†’ single space
        text = text.strip()
        
        # STEP 4: Ensure valid UTF-8
        text = text.encode("utf-8", "ignore").decode("utf-8")
        
        # STEP 5: Limit length for Pinecone (40KB metadata limit)
        max_length = 10000
        if len(text) > max_length:
            text = text[:max_length] + "..."
        
        return text
    
    @staticmethod
    def _extract_list_values(items: List[str]) -> str:
        """
        Convert list of strings to comma-separated string for Pinecone.
        
        Handles both list and string inputs safely.
        Also cleans escape sequences from the values.
        """
        if isinstance(items, list):
            # Clean each item and join
            cleaned_items = []
            for item in items:
                if item:
                    # Convert to string and clean escape sequences
                    item_str = str(item)
                    item_str = item_str.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', '')
                    item_str = item_str.strip()
                    if item_str:
                        cleaned_items.append(item_str)
            return ",".join(cleaned_items)
        elif isinstance(items, str):
            # Clean the string
            cleaned = items.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', '')
            return cleaned.strip()
        return ""

    async def _process_email_attachments(
        self,
        attachments_info: List[Dict],
        fetcher,
        email_id: str,
        max_size_mb: int,
        allowed_types: str,
    ) -> List[Dict]:
        """
        Process email attachments using unstructured.io.
        
        Performance: Processes in parallel when possible
        
        Returns:
            List of processed attachment dicts
        """
        from open_webui.utils.gmail_attachment_processor import GmailAttachmentProcessor
        
        att_processor = GmailAttachmentProcessor(
            max_size_mb=max_size_mb,
            allowed_extensions=allowed_types
        )
        
        return await att_processor.process_attachments_batch(
            attachments_info,
            fetcher,
            email_id
        )

