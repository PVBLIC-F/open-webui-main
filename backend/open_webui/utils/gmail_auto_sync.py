"""
Gmail Auto-Sync Orchestrator

This module orchestrates the automatic Gmail sync when users sign up with Google OAuth.

Flow:
1. User signs up/logs in with Google OAuth (with Gmail scopes)
2. OAuth callback triggers this auto-sync
3. Background task fetches all emails from Gmail
4. Processes and indexes to Pinecone using existing infrastructure
5. User's inbox is ready for semantic search

Components Used:
- GmailFetcher: Fetch emails from Gmail API
- GmailProcessor: Parse email data
- GmailIndexer: Chunk, embed, and format for Pinecone
- PineconeManager: Background upsert to Pinecone
- Uses existing chat summary infrastructure
"""

import logging
import asyncio
import time
import re
from typing import Dict, List, Optional
from datetime import datetime

from open_webui.utils.gmail_fetcher import GmailFetcher
from open_webui.utils.gmail_indexer import GmailIndexer
from open_webui.models.users import Users
from open_webui.models.oauth_sessions import OAuthSessions
from open_webui.models.gmail_sync import gmail_sync_status

# Set up logger with INFO level for visibility
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


class GmailAutoSync:
    """
    Orchestrates automatic Gmail sync for users.

    This class coordinates all Gmail components to perform a complete
    inbox sync in the background.
    """

    def __init__(
        self,
        embedding_service,
        content_aware_splitter,
        document_processor,
        pinecone_manager,
        gmail_namespace: str,
    ):
        """
        Initialize auto-sync orchestrator.

        Args:
            embedding_service: EmbeddingService from chat filter
            content_aware_splitter: ContentAwareTextSplitter from chat filter
            document_processor: DocumentProcessor from chat filter
            pinecone_manager: PineconeManager from chat filter
            gmail_namespace: Pinecone namespace for Gmail (PINECONE_NAMESPACE_GMAIL)
        """
        self.indexer = GmailIndexer(
            embedding_service=embedding_service,
            content_aware_splitter=content_aware_splitter,
            document_processor=document_processor,
            gmail_namespace=gmail_namespace,
        )
        self.pinecone = pinecone_manager
        self.gmail_namespace = gmail_namespace

        # Track active syncs
        self.active_syncs = {}  # user_id -> sync_status

    async def sync_user_gmail(
        self,
        user_id: str,
        oauth_token: dict,
        max_emails: int = 5000,
        skip_spam_trash: bool = True,
        incremental: bool = True,
    ) -> Dict:
        """
        Sync a user's Gmail inbox to Pinecone.

        Supports both full sync (first time) and incremental sync (subsequent runs).

        Args:
            user_id: User ID for isolation
            oauth_token: OAuth token dict with access_token
            max_emails: Maximum emails to sync (default: 5000)
            skip_spam_trash: Skip SPAM and TRASH folders (default: True)
            incremental: Whether to do incremental sync (default: True)

        Returns:
            Dict with sync statistics
        """

        logger.info("=" * 70)
        logger.info(f"üöÄ GMAIL SYNC STARTED - User: {user_id}")
        logger.info(f"   Max emails: {max_emails}")
        logger.info(f"   Skip spam/trash: {skip_spam_trash}")
        logger.info(f"   Incremental: {incremental}")
        logger.info(f"   Namespace: {self.gmail_namespace}")
        logger.info("=" * 70)

        # Get or create sync status
        sync_status = gmail_sync_status.get_sync_status(user_id)
        if not sync_status:
            sync_status = gmail_sync_status.create_sync_status(user_id)
            logger.info(f"Created new sync status for user {user_id}")
        else:
            logger.info(f"Found existing sync status for user {user_id}")
            logger.info(
                f"   Last sync: {datetime.fromtimestamp(sync_status.last_sync_timestamp) if sync_status.last_sync_timestamp else 'Never'}"
            )
            logger.info(f"   Total synced: {sync_status.total_emails_synced}")

        # Mark sync as starting
        gmail_sync_status.mark_sync_start(user_id)

        # Initialize sync status
        self.active_syncs[user_id] = {
            "status": "running",
            "start_time": time.time(),
            "total_emails": 0,
            "processed": 0,
            "indexed": 0,
            "errors": 0,
            "sync_type": "incremental" if incremental else "full",
            "last_sync_timestamp": sync_status.last_sync_timestamp,
        }

        try:
            # Validate OAuth token
            access_token = oauth_token.get("access_token")
            if not access_token:
                raise ValueError("OAuth token missing access_token")

            # Check for Gmail scopes
            token_scope = oauth_token.get("scope", "")
            if "gmail" not in token_scope:
                logger.warning(
                    f"User {user_id} OAuth token doesn't include Gmail scopes"
                )
                raise ValueError("OAuth token missing Gmail scopes")

            logger.info(f"‚úÖ OAuth token validated with Gmail scopes")

            # Step 1: Fetch emails from Gmail API
            logger.info(f"üì• Step 1: Fetching emails from Gmail API...")

            fetcher = GmailFetcher(
                oauth_token=access_token,
                max_requests_per_second=40,  # Conservative rate limit
                timeout=30,
            )

            # Determine sync query based on incremental mode
            if incremental and sync_status.last_sync_timestamp:
                # Incremental sync - get emails since last sync
                days_since_sync = (time.time() - sync_status.last_sync_timestamp) / (
                    24 * 3600
                )

                # Smart time window selection based on sync frequency
                if days_since_sync < 0.5:  # Less than 12 hours
                    query = "newer_than:6h"
                    logger.info(
                        f"   Incremental sync: fetching emails from last 6 hours"
                    )
                elif days_since_sync < 1:  # Less than 24 hours
                    query = "newer_than:1d"
                    logger.info(
                        f"   Incremental sync: fetching emails from last 24 hours"
                    )
                elif days_since_sync < 3:  # Less than 3 days
                    query = "newer_than:3d"
                    logger.info(
                        f"   Incremental sync: fetching emails from last 3 days"
                    )
                elif days_since_sync < 7:  # Less than a week
                    query = "newer_than:7d"
                    logger.info(
                        f"   Incremental sync: fetching emails from last 7 days"
                    )
                else:
                    # If last sync was much older, do a broader catch-up
                    query = "newer_than:30d"
                    logger.info(
                        f"   Incremental sync: fetching emails from last 30 days (catch-up)"
                    )

                # Adaptive batch size based on time window
                if days_since_sync < 1:
                    max_emails = min(max_emails, 500)  # Small batch for recent syncs
                elif days_since_sync < 7:
                    max_emails = min(max_emails, 1000)  # Medium batch for weekly syncs
                else:
                    max_emails = min(max_emails, 2000)  # Larger batch for catch-up

                logger.info(f"   Adaptive batch size: {max_emails} emails")
            else:
                # Full sync - get all emails
                query = None
                logger.info(
                    f"   Full sync: fetching all emails (first time or disabled incremental)"
                )

            emails, fetch_stats = await fetcher.fetch_all_emails(
                max_emails=max_emails,
                skip_spam_trash=skip_spam_trash,
                query=query,
                batch_size=100,
            )

            self.active_syncs[user_id]["total_emails"] = len(emails)

            # Analyze email types
            label_counts = {}
            for email in emails:
                labels = email.get("labelIds", [])
                for label in labels:
                    label_counts[label] = label_counts.get(label, 0) + 1

            logger.info("=" * 70)
            logger.info(f"‚úÖ FETCH COMPLETE")
            logger.info(f"   Total emails: {len(emails)}")
            logger.info(f"   API calls: {fetch_stats['api_calls']}")
            logger.info(f"   Fetch time: {fetch_stats.get('total_time', 0):.1f}s")
            logger.info(f"   Errors: {fetch_stats['errors']}")
            logger.info(f"\n   üìä Email Breakdown by Label:")
            for label, count in sorted(
                label_counts.items(), key=lambda x: x[1], reverse=True
            )[:10]:
                logger.info(f"      - {label}: {count} emails")
            if len(label_counts) > 10:
                logger.info(f"      - ... and {len(label_counts)-10} more labels")
            logger.info("=" * 70)

            if not emails:
                logger.info(f"üì≠ No emails found for user {user_id}")
                self.active_syncs[user_id]["status"] = "completed"
                return self._build_sync_result(user_id)

            # Step 2: Process and index emails to Pinecone
            logger.info("")
            logger.info("=" * 70)
            logger.info(f"‚öôÔ∏è PROCESSING & INDEXING - {len(emails)} emails")
            logger.info("=" * 70)

            indexed_count = await self._process_and_index_batch(
                emails=emails,
                user_id=user_id,
                batch_size=50,  # Reduced from 100 to prevent memory buildup
            )

            self.active_syncs[user_id]["indexed"] = indexed_count
            self.active_syncs[user_id]["status"] = "completed"

            # Mark sync as completed in database
            sync_duration = int(time.time() - self.active_syncs[user_id]["start_time"])
            gmail_sync_status.mark_sync_complete(
                user_id=user_id,
                emails_synced=indexed_count,
                duration_seconds=sync_duration,
                last_history_id=None,  # TODO: Track Gmail history ID for better incremental sync
                last_email_id=None,  # TODO: Track last email ID
            )

            # Clear emails list to free memory
            emails.clear()
            del emails

            result = self._build_sync_result(user_id)

            # Calculate performance metrics
            total_time = result["total_time"]
            emails_per_second = result["indexed"] / total_time if total_time > 0 else 0
            sync_type = self.active_syncs[user_id].get("sync_type", "unknown")

            logger.info("")
            logger.info("=" * 70)
            logger.info(f"üéâ GMAIL SYNC COMPLETED - User: {user_id}")
            logger.info(f"   Sync type: {sync_type.upper()}")
            logger.info(f"   Total emails fetched: {result['total_emails']}")
            logger.info(f"   Successfully indexed: {result['indexed']}")
            logger.info(
                f"   Duplicates skipped: {self.active_syncs[user_id].get('duplicates_skipped', 0)}"
            )
            logger.info(f"   Errors: {result['errors']}")
            logger.info(f"   Total time: {total_time:.1f}s")
            logger.info(f"   Processing rate: {emails_per_second:.1f} emails/sec")

            # Performance assessment
            if emails_per_second > 10:
                logger.info(
                    f"   Performance: üöÄ EXCELLENT ({emails_per_second:.1f} emails/sec)"
                )
            elif emails_per_second > 5:
                logger.info(
                    f"   Performance: ‚úÖ GOOD ({emails_per_second:.1f} emails/sec)"
                )
            else:
                logger.info(
                    f"   Performance: ‚ö†Ô∏è  SLOW ({emails_per_second:.1f} emails/sec)"
                )

            logger.info(f"   Status: ‚úÖ SUCCESS")
            logger.info("=" * 70)

            return result

        except Exception as e:
            logger.error(f"‚ùå Gmail sync failed for user {user_id}: {e}", exc_info=True)
            self.active_syncs[user_id]["status"] = "failed"
            self.active_syncs[user_id]["error"] = str(e)

            # Mark sync as failed in database
            gmail_sync_status.mark_sync_error(user_id, str(e))

            raise

    async def _process_and_index_batch(
        self,
        emails: List[dict],
        user_id: str,
        batch_size: int = 100,
    ) -> int:
        """
        Process emails and index to Pinecone in batches.

        Args:
            emails: List of Gmail API email responses
            user_id: User ID for isolation
            batch_size: Number of emails to process per batch

        Returns:
            Total number of emails successfully indexed
        """

        total_indexed = 0
        total_vectors = 0

        for i in range(0, len(emails), batch_size):
            batch = emails[i : i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(emails) + batch_size - 1) // batch_size
            progress_pct = int((i / len(emails)) * 100)

            logger.info("")
            logger.info(
                f"üì¶ Batch {batch_num}/{total_batches} ({progress_pct}% complete)"
            )
            logger.info(
                f"   Processing emails {i+1}-{min(i+batch_size, len(emails))} of {len(emails)}"
            )
            logger.info(f"   Batch size: {len(batch)} emails")

            # Process batch with GmailIndexer
            try:
                result = await self.indexer.process_email_batch(
                    emails=batch,
                    user_id=user_id,
                )

                upsert_data = result["upsert_data"]

                if upsert_data:
                    # Upsert to vector DB (collection-based isolation, not namespace)
                    await self.pinecone.schedule_upsert(
                        upsert_data,
                        namespace=None,  # Not used by Open WebUI's vector DB abstraction
                    )

                    total_indexed += result["processed"]
                    total_vectors += result["total_vectors"]

                    duplicates = result.get("duplicates_skipped", 0)
                    logger.info(f"   ‚úÖ Processed: {result['processed']} unique emails")
                    if duplicates > 0:
                        logger.info(
                            f"   ‚è≠Ô∏è  Skipped: {duplicates} duplicate emails (mass email deduplication)"
                        )
                    logger.info(f"   ‚úÖ Vectors created: {result['total_vectors']}")
                    logger.info(f"   ‚úÖ Errors: {result['errors']}")
                    logger.info(
                        f"   ‚úÖ Processing time: {result['processing_time']:.2f}s"
                    )
                    logger.info(
                        f"   üìä Running total: {total_indexed} emails, {total_vectors} vectors"
                    )

                    # Clear upsert_data to free memory after upserting
                    upsert_data.clear()
                    del result

                self.active_syncs[user_id]["processed"] = i + len(batch)

                # Clear processed batch to free memory
                batch.clear()

                # Yield to event loop after each batch to keep interface responsive
                await asyncio.sleep(0)  # Yield control to event loop immediately

                # Additional yield every few batches for better responsiveness
                if batch_num % 5 == 0:
                    await asyncio.sleep(0.01)  # Small delay every 5 batches

            except Exception as e:
                logger.error(f"Error processing batch {batch_num}: {e}")
                self.active_syncs[user_id]["errors"] += 1
                continue

        return total_indexed

    def _build_sync_result(self, user_id: str) -> Dict:
        """Build sync result with statistics"""

        sync_status = self.active_syncs.get(user_id, {})
        elapsed_time = time.time() - sync_status.get("start_time", time.time())

        return {
            "user_id": user_id,
            "status": sync_status.get("status", "unknown"),
            "total_emails": sync_status.get("total_emails", 0),
            "processed": sync_status.get("processed", 0),
            "indexed": sync_status.get("indexed", 0),
            "errors": sync_status.get("errors", 0),
            "total_time": elapsed_time,
            "error": sync_status.get("error"),
        }

    def get_sync_status(self, user_id: str) -> Optional[Dict]:
        """Get current sync status for a user"""

        if user_id not in self.active_syncs:
            return None

        return self._build_sync_result(user_id)


# ============================================================================
# TRIGGER FUNCTION (called from OAuth callback)
# ============================================================================


async def trigger_gmail_sync_if_needed(
    request,
    user_id: str,
    provider: str,
    token: dict,
    is_new_user: bool = False,
):
    """
    Trigger Gmail sync if conditions are met.

    This function is called from the OAuth callback handler.

    Args:
        request: FastAPI request object
        user_id: User ID who just authenticated
        provider: OAuth provider (e.g., "google")
        token: OAuth token dict
        is_new_user: True if this is first-time signup

    Conditions for triggering:
    - Provider must be "google"
    - Token must have Gmail scopes
    - ENABLE_GMAIL_AUTO_SYNC must be True
    - If GMAIL_AUTO_SYNC_ON_SIGNUP_ONLY, only trigger for new users
    """

    logger.info(
        f"\nüîç Gmail Sync Trigger Check - User: {user_id}, Provider: {provider}"
    )

    # Check if Gmail auto-sync is enabled
    if not request.app.state.config.ENABLE_GMAIL_AUTO_SYNC:
        logger.info("   ‚è≠Ô∏è  SKIP: Gmail auto-sync is disabled in config")
        return

    # Only trigger for Google OAuth
    if provider != "google":
        logger.info(f"   ‚è≠Ô∏è  SKIP: Not Google OAuth (provider: {provider})")
        return

    # Check for Gmail scopes
    token_scope = token.get("scope", "")
    has_gmail = "gmail" in token_scope
    logger.info(f"   üìß Gmail scope present: {has_gmail}")

    if not has_gmail:
        logger.info(f"   ‚è≠Ô∏è  SKIP: OAuth token doesn't include Gmail scopes")
        return

    # Check if we should only sync on signup
    sync_on_signup_only = request.app.state.config.GMAIL_AUTO_SYNC_ON_SIGNUP_ONLY
    logger.info(
        f"   üë§ New user: {is_new_user}, Sync on signup only: {sync_on_signup_only}"
    )

    if sync_on_signup_only and not is_new_user:
        logger.info(f"   ‚è≠Ô∏è  SKIP: Gmail auto-sync only on signup, user is not new")
        return

    # Check if admin has enabled Gmail sync for this user
    user = Users.get_user_by_id(user_id)
    if not user:
        logger.info(f"   ‚è≠Ô∏è  SKIP: User {user_id} not found")
        return
        
    # Check admin setting for Gmail sync
    admin_sync_enabled = getattr(user, 'gmail_sync_enabled', 0) == 1
    logger.info(f"   ‚öôÔ∏è  Admin Gmail sync setting: gmail_sync_enabled={admin_sync_enabled}")

    if not admin_sync_enabled:
        logger.info(f"   ‚è≠Ô∏è  SKIP: Admin has disabled Gmail sync for this user")
        logger.info(f"      Enable in Admin -> Users -> Edit User -> Gmail Email Sync")
        return
        
    # Check if user has enabled Gmail sync in their settings (additional user preference)
    if user and user.settings:
        settings_dict = (
            user.settings.model_dump()
            if hasattr(user.settings, "model_dump")
            else user.settings
        )
        gmail_settings = (
            settings_dict.get("gmail", {}) if isinstance(settings_dict, dict) else {}
        )
        user_sync_enabled = gmail_settings.get("sync_enabled", True)  # Default to enabled if admin allows
        logger.info(f"   ‚öôÔ∏è  User Gmail sync preference: sync_enabled={user_sync_enabled}")

        if not user_sync_enabled:
            logger.info(f"   ‚è≠Ô∏è  SKIP: User has disabled Gmail sync in their settings")
            return
    else:
        logger.info(f"   ‚öôÔ∏è  No user settings found - will sync (admin enabled, no user preference)")

    logger.info(f"   ‚úÖ ALL CONDITIONS MET - Triggering Gmail sync!")
    logger.info(
        f"\nüöÄ TRIGGERING AUTOMATIC GMAIL SYNC"
        f"\n   User: {user_id}"
        f"\n   New user: {is_new_user}"
        f"\n   Gmail scopes: ‚úì"
    )

    # Launch background task
    try:
        from open_webui.tasks import create_task

        # Create the background sync task
        task_id, task = await create_task(
            request.app.state.redis,
            _background_gmail_sync(request, user_id, token),
            id=f"gmail_sync_{user_id}",
        )

        logger.info(f"‚úÖ Gmail sync background task created: {task_id}")

    except Exception as e:
        logger.error(f"‚ùå Failed to create Gmail sync task for user {user_id}: {e}")


async def _background_gmail_sync(request, user_id: str, oauth_token: dict):
    """
    Background task that performs the actual Gmail sync.

    This runs asynchronously and doesn't block the OAuth callback.
    Integrates with existing Open WebUI RAG infrastructure.

    Args:
        request: FastAPI request object
        user_id: User ID to sync
        oauth_token: OAuth token dict with access_token
    """

    logger.info("\n" + "üî•" * 35)
    logger.info("üìß BACKGROUND GMAIL SYNC TASK STARTED")
    logger.info(f"   User ID: {user_id}")
    logger.info(f"   Task ID: gmail_sync_{user_id}")
    logger.info(f"   Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("üî•" * 35 + "\n")

    try:
        # Validate user
        user = Users.get_user_by_id(user_id)
        if not user:
            logger.error(f"User {user_id} not found")
            return

        # Get configuration
        max_emails = request.app.state.config.GMAIL_AUTO_SYNC_MAX_EMAILS
        skip_spam_trash = request.app.state.config.GMAIL_SKIP_SPAM_AND_TRASH
        batch_size = request.app.state.config.GMAIL_SYNC_BATCH_SIZE
        gmail_namespace = request.app.state.config.PINECONE_NAMESPACE_GMAIL

        logger.info(
            f"Gmail sync config: max_emails={max_emails}, "
            f"skip_spam_trash={skip_spam_trash}, batch_size={batch_size}, "
            f"namespace={gmail_namespace}"
        )

        # Create simple wrapper services for the background task
        # We'll use Open WebUI's existing embedding and Pinecone infrastructure

        class SimpleEmbeddingService:
            """Wrapper for Open WebUI's embedding function"""

            def __init__(self, app_state):
                self.app_state = app_state

            async def embed_batch(self, texts: List[str]) -> List[List[float]]:
                """Generate embeddings using Open WebUI's embedding function (non-blocking)"""
                from open_webui.utils.text_cleaner import TextCleaner

                async def embed_single(text: str) -> List[float]:
                    """Embed single text in executor to avoid blocking event loop"""
                    try:
                        # Run CPU-intensive cleaning and embedding in thread executor
                        loop = asyncio.get_running_loop()

                        # Clean and truncate in thread pool (CPU-intensive)
                        def clean_and_embed():
                            clean_text = TextCleaner.clean_text(text)

                            # Truncate to safe limit
                            max_chars = 8000
                            if len(clean_text) > max_chars:
                                clean_text = clean_text[:max_chars]

                            # Ensure not empty
                            if not clean_text or not clean_text.strip():
                                clean_text = "Email content not available"

                            # Call embedding function (may be network I/O)
                            return self.app_state.EMBEDDING_FUNCTION(
                                clean_text, prefix="", user=None
                            )

                        vector = await loop.run_in_executor(None, clean_and_embed)

                        if vector is None:
                            raise ValueError("Embedding function returned None")

                        return vector

                    except Exception as e:
                        logger.error(f"Embedding error: {e}")
                        # Fallback to zero vector
                        try:
                            dim = int(self.app_state.config.PINECONE_DIMENSION)
                        except:
                            dim = 1536
                        return [0.0] * dim

                # Process embeddings concurrently but with small batches to avoid overwhelming
                embeddings = []
                batch_size = 3  # Even smaller batch to keep event loop responsive

                for i in range(0, len(texts), batch_size):
                    batch = texts[i : i + batch_size]
                    batch_embeddings = await asyncio.gather(
                        *[embed_single(t) for t in batch]
                    )
                    embeddings.extend(batch_embeddings)

                    # Yield control to event loop between batches
                    await asyncio.sleep(0)

                    # Additional yield every few batches
                    if (i // batch_size) % 3 == 0:
                        await asyncio.sleep(0.001)  # Small delay every 3 batches

                return embeddings

        class SimpleTextSplitter:
            """Simple text splitter (fallback if ContentAwareTextSplitter not available)"""

            def split_text(self, text: str) -> List[str]:
                """Split text into chunks by paragraphs"""
                # Simple split by double newlines
                chunks = text.split("\n\n")
                chunks = [
                    c.strip() for c in chunks if c.strip() and len(c.strip()) > 50
                ]

                # If no chunks or single large chunk, return as-is
                if not chunks:
                    return [text] if text else []

                return chunks

        # Reuse quality scoring from gmail_indexer (no duplication!)
        from open_webui.utils.gmail_indexer import GmailIndexer

        class SimpleDocProcessor:
            """Thin wrapper to provide quality_score method"""

            @staticmethod
            def quick_quality_score(text: str) -> int:
                """Delegate to shared implementation"""
                # Use the same logic as everywhere else
                score = 0
                if len(text) > 200:
                    score += 1
                if text.count(".") >= 2:
                    score += 1
                if "\n\n" in text:
                    score += 1
                if not (text.count("\n- ") > 5 or text.count("\n‚Ä¢ ") > 5):
                    score += 1
                if re.search(
                    r"\b(because|therefore|however|additionally)\b", text, re.IGNORECASE
                ):
                    score += 1
                return score

        class SimplePineconeManager:
            """
            Thin wrapper around VECTOR_DB_CLIENT for Gmail email storage.

            Uses collection-based isolation (not namespaces):
            - Each user has their own collection: gmail_{user_id}
            - User isolation via collection name + metadata user_id
            - Compatible with Open WebUI's vector DB abstraction
            """

            def __init__(self, namespace):
                self.namespace = namespace  # Not used (Open WebUI doesn't support namespaces in abstraction)

            async def schedule_upsert(
                self, upsert_data: List[dict], namespace: str = None
            ):
                """
                Upsert vectors using existing VECTOR_DB_CLIENT (from pinecone.py).

                User Isolation Strategy:
                - Collection name: gmail_{user_id} (primary isolation)
                - Metadata user_id: {user_id} (secondary filter)
                - Type metadata: "email" (document type filter)

                Runs in thread executor to avoid blocking the async event loop.
                """
                from open_webui.retrieval.vector.factory import VECTOR_DB_CLIENT

                # VECTOR_DB_CLIENT.upsert expects List[dict] with keys: id, text, vector, metadata
                # Filter out zero vectors (Pinecone rejects them)
                valid_items = []
                zero_vector_count = 0

                for item in upsert_data:
                    vector = item["values"]
                    # Check if vector has at least one non-zero value
                    if any(v != 0.0 for v in vector):
                        # Handle both dict and VectorItem object cases
                        metadata = item["metadata"]
                        if hasattr(metadata, "get"):
                            # It's a dictionary
                            chunk_text = metadata.get("chunk_text", "")
                            metadata_dict = metadata
                        else:
                            # It's a VectorItem object or similar - access as attributes
                            chunk_text = getattr(metadata, "chunk_text", "")
                            metadata_dict = (
                                metadata.__dict__
                                if hasattr(metadata, "__dict__")
                                else metadata
                            )

                        valid_items.append(
                            {
                                "id": item["id"],
                                "text": chunk_text,
                                "vector": vector,
                                "metadata": metadata_dict,
                            }
                        )
                    else:
                        zero_vector_count += 1
                        # Handle both dict and VectorItem object cases for email_id
                        metadata = item["metadata"]
                        if hasattr(metadata, "get"):
                            email_id = metadata.get("email_id", "unknown")
                        else:
                            email_id = getattr(metadata, "email_id", "unknown")
                        logger.warning(
                            f"‚ö†Ô∏è  Skipping zero vector for email {email_id} (embedding failed)"
                        )

                if zero_vector_count > 0:
                    logger.warning(
                        f"‚ö†Ô∏è  Skipped {zero_vector_count} vectors with all zeros"
                    )

                if not valid_items:
                    logger.warning(
                        "‚ö†Ô∏è  No valid vectors in batch (all embeddings failed), skipping upsert"
                    )
                    return

                # Collection-based isolation: each user has their own collection
                # Note: Open WebUI uses collections for user isolation, not namespaces
                collection_name = f"gmail_{user_id}"

                logger.info(
                    f"üì§ Upserting {len(valid_items)} valid vectors to collection: {collection_name}"
                )
                logger.info(
                    f"   Collection-based isolation (not namespace) - each user gets their own collection"
                )

                try:
                    # Use existing VECTOR_DB_CLIENT.upsert (handles batching, errors, retries)
                    # Run in thread executor to avoid blocking async event loop
                    loop = asyncio.get_running_loop()
                    await loop.run_in_executor(
                        None, VECTOR_DB_CLIENT.upsert, collection_name, valid_items
                    )
                    logger.info(
                        f"‚úÖ Upserted {len(valid_items)} vectors to collection {collection_name}"
                    )

                    # Clear valid_items to free memory immediately after upsert
                    valid_items.clear()

                except Exception as e:
                    logger.error(f"‚ùå Vector DB upsert error: {e}")
                    # Clear memory even on error
                    valid_items.clear()
                    raise

        # Initialize services
        embedding_service = SimpleEmbeddingService(request.app.state)
        text_splitter = SimpleTextSplitter()
        doc_processor = SimpleDocProcessor()
        pinecone_manager = SimplePineconeManager(gmail_namespace)

        # Initialize GmailAutoSync orchestrator
        gmail_sync = GmailAutoSync(
            embedding_service=embedding_service,
            content_aware_splitter=text_splitter,
            document_processor=doc_processor,
            pinecone_manager=pinecone_manager,
            gmail_namespace=gmail_namespace,
        )

        # Execute the sync
        logger.info(f"üöÄ Starting Gmail sync for user {user_id}...")

        result = await gmail_sync.sync_user_gmail(
            user_id=user_id,
            oauth_token=oauth_token,
            max_emails=max_emails,
            skip_spam_trash=skip_spam_trash,
            incremental=True,  # Enable incremental sync by default
        )

        logger.info("\n" + "üéâ" * 35)
        logger.info("‚úÖ BACKGROUND GMAIL SYNC TASK COMPLETED")
        logger.info(f"   User ID: {user_id}")
        logger.info(f"   Emails indexed: {result['indexed']}")
        logger.info(f"   Total time: {result['total_time']:.1f}s")
        logger.info(f"   Status: SUCCESS")
        logger.info("üéâ" * 35 + "\n")

    except Exception as e:
        logger.error("\n" + "‚ùå" * 35)
        logger.error(f"‚ùå BACKGROUND GMAIL SYNC TASK FAILED - User: {user_id}")
        logger.error(f"   Error: {str(e)}")
        logger.error("‚ùå" * 35 + "\n")
        logger.error(f"Full traceback:", exc_info=True)


# ============================================================================
# TESTING
# ============================================================================


async def test_auto_sync_orchestrator():
    """Test the auto-sync orchestrator logic"""

    print("\n" + "=" * 60)
    print("Phase 5 - Auto-Sync Orchestrator Test")
    print("=" * 60)

    # Mock services
    class MockEmbeddingService:
        async def embed_batch(self, texts):
            return [[0.1] * 1536 for _ in texts]

    class MockSplitter:
        def split_text(self, text):
            return [text]  # Simple: one chunk per email

    class MockDocProcessor:
        @staticmethod
        def quick_quality_score(text):
            return 4

    class MockPineconeManager:
        async def schedule_upsert(self, data, namespace):
            logger.info(f"Mock upsert: {len(data)} vectors to namespace '{namespace}'")
            return f"job_{time.time()}"

    print("\n‚úÖ TEST 1: Initialize Auto-Sync Orchestrator")

    sync = GmailAutoSync(
        embedding_service=MockEmbeddingService(),
        content_aware_splitter=MockSplitter(),
        document_processor=MockDocProcessor(),
        pinecone_manager=MockPineconeManager(),
        gmail_namespace="gmail-inbox",
    )

    print(f"   Namespace: {sync.gmail_namespace}")
    print(f"   Indexer initialized: ‚úì")
    print(f"   Pinecone manager ready: ‚úì")
    print("   ‚úÖ PASSED\n")

    print("‚úÖ TEST 2: Sync Status Tracking")

    # Simulate starting a sync
    test_user_id = "test_user_999"
    sync.active_syncs[test_user_id] = {
        "status": "running",
        "start_time": time.time(),
        "total_emails": 100,
        "processed": 50,
        "indexed": 45,
        "errors": 5,
    }

    status = sync.get_sync_status(test_user_id)

    print(f"   User: {status['user_id']}")
    print(f"   Status: {status['status']}")
    print(f"   Processed: {status['processed']}/{status['total_emails']}")
    print(f"   Indexed: {status['indexed']}")
    print(f"   Errors: {status['errors']}")

    assert status["status"] == "running", "Status should be running"
    assert status["processed"] == 50, "Should track processed count"

    print("   ‚úÖ PASSED\n")

    print("=" * 60)
    print("Phase 5 Orchestrator Tests Complete ‚úÖ")
    print("=" * 60)

    print("\nOrchestrator capabilities:")
    print("  ‚úÖ Coordinates all Gmail components")
    print("  ‚úÖ Tracks sync progress per user")
    print("  ‚úÖ Integrates with existing services")
    print("  ‚úÖ Ready for OAuth callback integration")

    return True


if __name__ == "__main__":
    print("Testing Phase 5 - Gmail Auto-Sync Orchestrator")
    success = asyncio.run(test_auto_sync_orchestrator())

    if success:
        print("\nüéâ Phase 5 orchestrator tests PASSED!")
        print("\nNext: Hook into OAuth callback (Phase 5.4)")

    exit(0 if success else 1)
